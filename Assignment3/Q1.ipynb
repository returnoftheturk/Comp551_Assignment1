{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from numpy.random import random_integers\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTxt(fileName):\n",
    "    fullFileName = r'C:\\Users\\Owner\\McGill\\4thYear\\COMP551\\Assignments\\Assignment3\\Datasets' + fileName\n",
    "    df = pd.read_csv(fullFileName, encoding='utf-8', header = None,\n",
    "                 sep='\\t')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readCsv(fileName):\n",
    "    fullFileName = r'C:\\Users\\Owner\\McGill\\4thYear\\COMP551\\Assignments\\Assignment3' + fileName\n",
    "    df = pd.read_csv(fullFileName, encoding='utf-8', header = None,\n",
    "                 sep=',')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toCsvDf(fileName, df):\n",
    "    fullFileName = r'C:\\Users\\Owner\\McGill\\4thYear\\COMP551\\Assignments\\Assignment3\\Datasets' + fileName\n",
    "    df.to_csv(fullFileName, header = False, sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toCsvNp(fileName, npArray):\n",
    "    fullFileName = r'C:\\Users\\Owner\\McGill\\4thYear\\COMP551\\Assignments\\Assignment3\\Datasets' + fileName\n",
    "    np.savetxt(fullFileName, npArray, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)\n",
    "print(string.punctuation.replace('\\'', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 201173), ('and', 98176), ('a', 97789), ('of', 87191), ('to', 81487), ('is', 64545), ('br', 61429), ('in', 56016), ('it', 47572)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (10000,1) into shape (9999,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-38e7d7706048>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mcreateDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'IMDB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-38e7d7706048>\u001b[0m in \u001b[0;36mcreateDictionary\u001b[1;34m(trainingSet)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#     print(dictionary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mnewArray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mwithIndexVocabArray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewArray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrainingSet\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'IMDB'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(arr, obj, values, axis)\u001b[0m\n\u001b[0;32m   5057\u001b[0m         \u001b[0mnew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslobj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslobj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5058\u001b[0m         \u001b[0mslobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnumnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5059\u001b[1;33m         \u001b[0mnew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslobj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5060\u001b[0m         \u001b[0mslobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnumnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5061\u001b[0m         \u001b[0mslobj2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (10000,1) into shape (9999,1)"
     ]
    }
   ],
   "source": [
    "def createDictionary(trainingSet):\n",
    "    allWords = list()\n",
    "    cnt = Counter()\n",
    "    \n",
    "    if trainingSet=='IMDB':\n",
    "        trainingDF = readTxt('\\IMDB-train.txt')\n",
    "    elif trainingSet=='yelp':\n",
    "        trainingDF = readTxt('\\yelp-train.txt')\n",
    "    \n",
    "# Replacing !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ with ' ' * 31 (31 spaces, needs to be same length)\n",
    "# and replacing ' with ' ' (apostrophe with space)\n",
    "    translator = str.maketrans(string.punctuation.replace('\\'', ''), 31*' ', '\\'')\n",
    "    \n",
    "    for i in range(0, len(trainingDF)):\n",
    "        allWords.extend(trainingDF.iloc[i,0].translate(translator).lower().split(\" \"))\n",
    "    \n",
    "    for word in allWords:\n",
    "        cnt[word] +=1\n",
    "    print(cnt.most_common(10)[1:])\n",
    "    dictionaryWords = list(zip(*cnt.most_common(10001)[1:]))[0]\n",
    "    \n",
    "    dictionary = {}\n",
    "    for index, key in enumerate(dictionaryWords):\n",
    "        dictionary[key] = index\n",
    "#     print(dictionary)\n",
    "    newArray = np.asarray(cnt.most_common(10000)[1:])\n",
    "    withIndexVocabArray = np.insert(newArray, 1, range(0,10000),1)\n",
    "    \n",
    "    if trainingSet=='IMDB':\n",
    "        toCsvDf('\\IMDB-vocab.txt', pd.DataFrame(withIndexVocabArray))\n",
    "    elif trainingSet=='yelp':\n",
    "        toCsvDf('\\yelp-vocab.txt', pd.DataFrame(withIndexVocabArray))\n",
    "        \n",
    "    return dictionary\n",
    "createDictionary('IMDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertWordsToVector(trainingRow, dictionary, BOWType):\n",
    "    translator = str.maketrans('','', string.punctuation)\n",
    "    returnRow = trainingRow.translate(translator).lower().split(\" \")\n",
    "    vector = np.zeros(10000, dtype = np.int8)\n",
    "    \n",
    "    for word in returnRow:\n",
    "        if word in dictionary:\n",
    "            if BOWType == 'BagOfWords':\n",
    "                vector[dictionary[word]] = np.int8(1)\n",
    "            elif BOWType == 'Frequency':\n",
    "                vector[dictionary[word]] += 1\n",
    "    \n",
    "    if BOWType == 'Frequency':\n",
    "#         To accomodate for the fact that one of the rows has one word, d-gust-ing, and the lenght of the vector is zero.\n",
    "#         Hence, the vector returns [Nan Nan Nan ... Nan Nan].  Now it returns [0 0 0 ... 0 0 0]\n",
    "        vectorLength =  np.sum(vector)\n",
    "        if vectorLength>0:\n",
    "            vector = np.divide(vector, vectorLength)\n",
    "            \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05434783  0.02173913  0.00543478 ...,  0.          0.          0.        ]\n",
      " [ 0.03488372  0.03488372  0.0872093  ...,  0.          0.          0.        ]\n",
      " [ 0.05555556  0.03703704  0.04938272 ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.03333333  0.075       0.04166667 ...,  0.          0.          0.        ]\n",
      " [ 0.08029197  0.03832117  0.05291971 ...,  0.          0.          0.        ]\n",
      " [ 0.02777778  0.05555556  0.         ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def createBagOfWordsMatrix(df, dictionary, BOWType):\n",
    "    translator = str.maketrans('','', string.punctuation)\n",
    "    \n",
    "    if BOWType == 'BagOfWords':\n",
    "        distancesArray = np.zeros((len(df), len(dictionary)), dtype = np.int8)\n",
    "    elif BOWType == 'Frequency':\n",
    "        distancesArray = np.zeros((len(df), len(dictionary)))\n",
    "        \n",
    "    for i in range(0, len(df)):\n",
    "        vector = convertWordsToVector(df.iloc[i,0], dictionary, BOWType)\n",
    "        distancesArray[i] = vector\n",
    "#         print(np.sum(distancesArray[i]))\n",
    "    return distancesArray\n",
    "# trainingDF = \n",
    "# npArray = createBagOfWordsMatrix(readTxt('\\yelp-train.txt'), createDictionary('yelp'), 'Frequency')\n",
    "print(createBagOfWordsMatrix(readTxt('\\yelp-train.txt'), createDictionary('yelp'), 'Frequency'))\n",
    "# dfArray = pd.DataFrame(npArray)\n",
    "# dfArray.to_csv(r'C:\\Users\\Owner\\McGill\\4thYear\\COMP551\\Assignments\\Assignment3\\Datasets\\TESTING2.csv', header = False, sep=',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i cant believe i havent yelped about the place yet several months maybe over a year ago my husband read a newspaper article about the clover coffee maker and the one place in town that had managed to one i was skeptical as is my nature it cant be that much better right youre just saying its amazing because you want to talk about the new hot coffee shop you discovered right well maybe but i love this place and i dont think it has a whole lot to do with the clover they roast their own beans and they roast them way differently than that other ginormous coffee chain  all a light or medium roast never bitter never oily never yucky the coffee they make there is obviously the best but i send my husband in every week now to buy a pound of beans so that i can the same coffee at home add an edgy though sometimes intimidating seating area great local art which we bought off the wall and smiley sold cant wait to try out the downtown location \n",
      "best nights to go to postinos are mondays and tuesdays  they offer 20 deals where you get 4 slices of bruschetta out of the 12 that they offer and one whole bottle of the house wine  each bruschetta slice is probably the size of maybe your hand with your fingers  if youre a petite girl     they then cut each slice into 4s  perfect for went on a monday night after 8pm and ordered 2 bottles of wine the 2 orders of the bruschetta which were 8 slices for those that cant count and a bowl of olives  the total came out to about a little over wo tip  awesome    plus they have complementary valet parking everything was just fantastic ive never had fig before and they made my experience there quite memorable  i definitely recommend you come in regardless if its for their 20 deals on or not  theyve got great food and i will be back \n"
     ]
    }
   ],
   "source": [
    "def createReviewsTxt(df, dictionary):\n",
    "    translator = str.maketrans('','',string.punctuation)\n",
    "    \n",
    "    \n",
    "    for i in range(0, 2):\n",
    "        reviewsString = ''\n",
    "        trainingRow = df.iloc[i,0].translate(translator).lower().split(\" \")\n",
    "        for word in trainingRow:\n",
    "            if word in dictionary:\n",
    "                reviewsString += word + ' '\n",
    "        print(reviewsString)\n",
    "            \n",
    "createReviewsTxt(readTxt('\\yelp-train.txt'), createDictionary('yelp'))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testScores(predicted, actual):\n",
    "    print('F1 Score:', f1_score(actual, predicted, average='macro'))\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp Random Classifier\n",
      "Training F1-Measure\n",
      "F1 Score: 0.184123905128\n",
      "Confusion Matrix:\n",
      " [[107 106  96 119  94]\n",
      " [124 129 115 137 136]\n",
      " [202 203 199 199 194]\n",
      " [503 478 484 497 506]\n",
      " [473 462 483 464 490]]\n",
      "Validation F1-Measure\n",
      "F1 Score: 0.176673463167\n",
      "Confusion Matrix:\n",
      " [[14 15 23 18 14]\n",
      " [24 16 20 12 24]\n",
      " [25 35 37 34 33]\n",
      " [62 68 82 74 70]\n",
      " [58 48 70 71 53]]\n",
      "Test F1-Measure\n",
      "F1 Score: 0.18894982874\n",
      "Confusion Matrix:\n",
      " [[ 27  29  25  33  29]\n",
      " [ 37  40  38  36  39]\n",
      " [ 63  54  60  65  58]\n",
      " [126 164 135 145 132]\n",
      " [122 127 133 136 147]]\n"
     ]
    }
   ],
   "source": [
    "def randomClassifier(dataset):\n",
    "    print(dataset, 'Random Classifier')\n",
    "    if dataset == 'yelp':\n",
    "        dictionary = createDictionary('yelp')\n",
    "        trainingDF = readTxt('\\yelp-train.txt')\n",
    "        validDF = readTxt('\\yelp-valid.txt')\n",
    "        testDF = readTxt('\\yelp-test.txt')\n",
    "    elif dataset == 'IMDB':\n",
    "        dictionary = createDictionary('IMDB')\n",
    "        trainingDF = readTxt('\\IMDB-train.txt')\n",
    "        validDF = readTxt('\\IMDB-valid.txt')\n",
    "        testDF = readTxt('\\IMDB-test.txt')\n",
    "    \n",
    "    randomClassifier = DummyClassifier(strategy='uniform')\n",
    "    randomClassifier.fit(createBagOfWordsMatrix(trainingDF, dictionary), trainingDF[1])\n",
    "    \n",
    "    print('Training F1-Measure')\n",
    "    predictionsArray = randomClassifier.predict(createBagOfWordsMatrix(trainingDF, dictionary))\n",
    "    testScores(predictionsArray, trainingDF[1])\n",
    "    \n",
    "    print('Validation F1-Measure')\n",
    "    predictionsArray = randomClassifier.predict(createBagOfWordsMatrix(validDF, dictionary))\n",
    "    testScores(predictionsArray, validDF[1])\n",
    "    \n",
    "    print('Test F1-Measure')\n",
    "    predictionsArray = randomClassifier.predict(createBagOfWordsMatrix(testDF, dictionary))\n",
    "    testScores(predictionsArray, testDF[1])\n",
    "    \n",
    "randomClassifier('yelp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp Majority Classifier\n",
      "Training F1-Measure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.104267004647\n",
      "Confusion Matrix:\n",
      " [[   0    0    0  522    0]\n",
      " [   0    0    0  641    0]\n",
      " [   0    0    0  997    0]\n",
      " [   0    0    0 2468    0]\n",
      " [   0    0    0 2372    0]]\n",
      "Validation F1-Measure\n",
      "F1 Score: 0.105014749263\n",
      "Confusion Matrix:\n",
      " [[  0   0   0  84   0]\n",
      " [  0   0   0  96   0]\n",
      " [  0   0   0 164   0]\n",
      " [  0   0   0 356   0]\n",
      " [  0   0   0 300   0]]\n",
      "Test F1-Measure\n",
      "F1 Score: 0.103923019985\n",
      "Confusion Matrix:\n",
      " [[  0   0   0 143   0]\n",
      " [  0   0   0 190   0]\n",
      " [  0   0   0 300   0]\n",
      " [  0   0   0 702   0]\n",
      " [  0   0   0 665   0]]\n"
     ]
    }
   ],
   "source": [
    "def majorityClassifier(dataset):\n",
    "    print(dataset, 'Majority Classifier')\n",
    "    if dataset == 'yelp':\n",
    "        dictionary = createDictionary('yelp')\n",
    "        trainingDF = readTxt('\\yelp-train.txt')\n",
    "        validDF = readTxt('\\yelp-valid.txt')\n",
    "        testDF = readTxt('\\yelp-test.txt')\n",
    "    elif dataset == 'IMDB':\n",
    "        dictionary = createDictionary('IMDB')\n",
    "        trainingDF = readTxt('\\IMDB-train.txt')\n",
    "        validDF = readTxt('\\IMDB-valid.txt')\n",
    "        testDF = readTxt('\\IMDB-test.txt')\n",
    "    \n",
    "    majorityClassifier = DummyClassifier(strategy='most_frequent')\n",
    "    majorityClassifier.fit(createBagOfWordsMatrix(trainingDF, dictionary), trainingDF[1])\n",
    "    \n",
    "    print('Training F1-Measure')\n",
    "    predictionsArray = majorityClassifier.predict(createBagOfWordsMatrix(trainingDF, dictionary))\n",
    "    testScores(predictionsArray, trainingDF[1])\n",
    "    \n",
    "    print('Validation F1-Measure')\n",
    "    predictionsArray = majorityClassifier.predict(createBagOfWordsMatrix(validDF, dictionary))\n",
    "    testScores(predictionsArray, validDF[1])\n",
    "    \n",
    "    print('Test F1-Measure')\n",
    "    predictionsArray = majorityClassifier.predict(createBagOfWordsMatrix(testDF, dictionary))\n",
    "    testScores(predictionsArray, testDF[1])\n",
    "    \n",
    "# baselineClassifier('IMDB')\n",
    "majorityClassifier('yelp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB\n",
      "Training F1-Measure\n",
      "F1 Score: 0.871295845531\n",
      "Confusion Matrix:\n",
      " [[6663  837]\n",
      " [1093 6407]]\n",
      "Validation F1-Measure\n",
      "F1 Score: 0.842861266874\n",
      "Confusion Matrix:\n",
      " [[4293  707]\n",
      " [ 864 4136]]\n",
      "Test F1-Measure\n",
      "F1 Score: 0.835917809086\n",
      "Confusion Matrix:\n",
      " [[10844  1656]\n",
      " [ 2442 10058]]\n"
     ]
    }
   ],
   "source": [
    "def bernoulliNB(dataset):\n",
    "    print(dataset)\n",
    "    if dataset == 'yelp':\n",
    "        dictionary = createDictionary('yelp')\n",
    "        trainingDF = readTxt('\\yelp-train.txt')\n",
    "        validDF = readTxt('\\yelp-valid.txt')\n",
    "        testDF = readTxt('\\yelp-test.txt')\n",
    "    elif dataset == 'IMDB':\n",
    "        dictionary = createDictionary('IMDB')\n",
    "        trainingDF = readTxt('\\IMDB-train.txt')\n",
    "        validDF = readTxt('\\IMDB-valid.txt')\n",
    "        testDF = readTxt('\\IMDB-test.txt')\n",
    "        \n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(createBagOfWordsMatrix(trainingDF, dictionary, 'Frequency'), trainingDF[1])\n",
    "    \n",
    "    print('Training F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(trainingDF, dictionary, 'Frequency'))\n",
    "    testScores(predictionsArray, trainingDF[1])\n",
    "    \n",
    "    print('Validation F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(validDF, dictionary, 'Frequency'))\n",
    "    testScores(predictionsArray, validDF[1])\n",
    "    \n",
    "    print('Test F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(testDF, dictionary, 'Frequency'))\n",
    "    testScores(predictionsArray, testDF[1])\n",
    "    \n",
    "bernoulliNB('IMDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp\n",
      "Training F1-Measure\n",
      "F1 Score: 1.0\n",
      "Confusion Matrix:\n",
      " [[ 522    0    0    0    0]\n",
      " [   0  641    0    0    0]\n",
      " [   0    0  997    0    0]\n",
      " [   0    0    0 2468    0]\n",
      " [   0    0    0    0 2372]]\n",
      "Validation F1-Measure\n",
      "F1 Score: 0.279822135371\n",
      "Confusion Matrix:\n",
      " [[ 19  13  14  14  24]\n",
      " [ 13  11  17  24  31]\n",
      " [  8  21  40  58  37]\n",
      " [ 19  31  57 137 112]\n",
      " [ 15  17  31 110 127]]\n",
      "Test F1-Measure\n",
      "F1 Score: 0.276493554759\n",
      "Confusion Matrix:\n",
      " [[ 24  24  27  31  37]\n",
      " [ 22  28  42  57  41]\n",
      " [ 11  33  59 133  64]\n",
      " [ 37  44  91 278 252]\n",
      " [ 25  35  60 237 308]]\n"
     ]
    }
   ],
   "source": [
    "def decisionTree(dataset):\n",
    "    print(dataset)\n",
    "    if dataset == 'yelp':\n",
    "        dictionary = createDictionary('yelp')\n",
    "        trainingDF = readTxt('\\yelp-train.txt')\n",
    "        validDF = readTxt('\\yelp-valid.txt')\n",
    "        testDF = readTxt('\\yelp-test.txt')\n",
    "    elif dataset == 'IMDB':\n",
    "        dictionary = createDictionary('IMDB')\n",
    "        trainingDF = readTxt('\\IMDB-train.txt')\n",
    "        validDF = readTxt('\\IMDB-valid.txt')\n",
    "        testDF = readTxt('\\IMDB-test.txt')\n",
    "        \n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(createBagOfWordsMatrix(trainingDF, dictionary), trainingDF[1])\n",
    "    \n",
    "    print('Training F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(trainingDF, dictionary))\n",
    "    testScores(predictionsArray, trainingDF[1])\n",
    "    \n",
    "    print('Validation F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(validDF, dictionary))\n",
    "    testScores(predictionsArray, validDF[1])\n",
    "    \n",
    "    print('Test F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(testDF, dictionary))\n",
    "    testScores(predictionsArray, testDF[1])\n",
    "    \n",
    "decisionTree('yelp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp\n",
      "Training F1-Measure\n",
      "F1 Score: 0.997870801179\n",
      "Confusion Matrix:\n",
      " [[ 520    0    0    0    2]\n",
      " [   0  640    0    0    1]\n",
      " [   0    0  996    1    0]\n",
      " [   0    0    0 2454   14]\n",
      " [   0    0    0    2 2370]]\n",
      "Validation F1-Measure\n",
      "F1 Score: 0.413428426203\n",
      "Confusion Matrix:\n",
      " [[ 36  23   8   9   8]\n",
      " [ 14  25  15  28  14]\n",
      " [  7  23  50  65  19]\n",
      " [ 10  11  32 170 133]\n",
      " [  4   9  18 102 167]]\n",
      "Test F1-Measure\n",
      "F1 Score: 0.401651918294\n",
      "Confusion Matrix:\n",
      " [[ 56  33  15  19  20]\n",
      " [ 33  58  41  36  22]\n",
      " [ 12  42  77 118  51]\n",
      " [ 12  31  84 340 235]\n",
      " [ 15  12  40 237 361]]\n"
     ]
    }
   ],
   "source": [
    "def linearSVC(dataset):\n",
    "    print(dataset)\n",
    "    if dataset == 'yelp':\n",
    "        dictionary = createDictionary('yelp')\n",
    "        trainingDF = readTxt('\\yelp-train.txt')\n",
    "        validDF = readTxt('\\yelp-valid.txt')\n",
    "        testDF = readTxt('\\yelp-test.txt')\n",
    "    elif dataset == 'IMDB':\n",
    "        dictionary = createDictionary('IMDB')\n",
    "        trainingDF = readTxt('\\IMDB-train.txt')\n",
    "        validDF = readTxt('\\IMDB-valid.txt')\n",
    "        testDF = readTxt('\\IMDB-test.txt')\n",
    "        \n",
    "    clf = LinearSVC()\n",
    "    clf.fit(createBagOfWordsMatrix(trainingDF, dictionary), trainingDF[1])\n",
    "    \n",
    "    print('Training F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(trainingDF, dictionary))\n",
    "    testScores(predictionsArray, trainingDF[1])\n",
    "    \n",
    "    print('Validation F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(validDF, dictionary))\n",
    "    testScores(predictionsArray, validDF[1])\n",
    "    \n",
    "    print('Test F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(testDF, dictionary))\n",
    "    testScores(predictionsArray, testDF[1])\n",
    "    \n",
    "linearSVC('yelp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
