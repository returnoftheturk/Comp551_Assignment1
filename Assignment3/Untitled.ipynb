{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from numpy.random import random_integers\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTxt(fileName):\n",
    "    fullFileName = r'C:\\Users\\Owner\\McGill\\4thYear\\COMP551\\Assignments\\Assignment3\\Datasets' + fileName\n",
    "    df = pd.read_csv(fullFileName, encoding='utf-8', header = None,\n",
    "                 sep='\\t')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readCsv(fileName):\n",
    "    fullFileName = r'C:\\Users\\Owner\\McGill\\4thYear\\COMP551\\Assignments\\Assignment3' + fileName\n",
    "    df = pd.read_csv(fullFileName, encoding='utf-8', header = None,\n",
    "                 sep=',')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toCsvDf(fileName, df):\n",
    "    fullFileName = r'C:\\Users\\Owner\\McGill\\4thYear\\COMP551\\Assignments\\Assignment3\\Datasets' + fileName\n",
    "    df.to_csv(fullFileName, header = False, sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toCsvNp(fileName, npArray):\n",
    "    fullFileName = r'C:\\Users\\Owner\\McGill\\4thYear\\COMP551\\Assignments\\Assignment3\\Datasets' + fileName\n",
    "    np.savetxt(fullFileName, npArray, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDictionary(trainingSet):\n",
    "    allWords = list()\n",
    "    cnt = Counter()\n",
    "    \n",
    "    if trainingSet=='IMDB':\n",
    "        trainingDF = readTxt('\\IMDB-train.txt')\n",
    "    elif trainingSet=='yelp':\n",
    "        trainingDF = readTxt('\\yelp-train.txt')\n",
    "    \n",
    "# Replacing !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ with ' ' * 31 (31 spaces, needs to be same length)\n",
    "# and replacing ' with ' ' (apostrophe with space)\n",
    "    translator = str.maketrans(string.punctuation.replace('\\'', ''), 31*' ', '\\'')\n",
    "    \n",
    "    for i in range(0, len(trainingDF)):\n",
    "        allWords.extend(trainingDF.iloc[i,0].translate(translator).lower().split(\" \"))\n",
    "    \n",
    "    for word in allWords:\n",
    "        cnt[word] +=1\n",
    "    \n",
    "    dictionaryWords = list(zip(*cnt.most_common(10001)[1:]))[0]\n",
    "    \n",
    "    dictionary = {}\n",
    "    for index, key in enumerate(dictionaryWords):\n",
    "        dictionary[key] = index\n",
    "#     print(dictionary)\n",
    "    newArray = np.asarray(cnt.most_common(10001)[1:])\n",
    "    withIndexVocabArray = np.insert(newArray, 1, range(0,10000),1)\n",
    "    \n",
    "#     if trainingSet=='IMDB':\n",
    "#         toCsvDf('\\IMDB-vocab.txt', pd.DataFrame(withIndexVocabArray))\n",
    "#     elif trainingSet=='yelp':\n",
    "#         toCsvDf('\\yelp-vocab.txt', pd.DataFrame(withIndexVocabArray))\n",
    "        \n",
    "    return dictionary\n",
    "# createDictionary('yelp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertWordsToVector(trainingRow, dictionary, BOWType):\n",
    "    translator = str.maketrans(string.punctuation.replace('\\'', ''), 31*' ', '\\'')\n",
    "    returnRow = trainingRow.translate(translator).lower().split(\" \")\n",
    "    vector = np.zeros(10000, dtype = np.int8)\n",
    "    \n",
    "    for word in returnRow:\n",
    "        if word in dictionary:\n",
    "            if BOWType == 'BagOfWords':\n",
    "                vector[dictionary[word]] = np.int8(1)\n",
    "            elif BOWType == 'Frequency':\n",
    "                vector[dictionary[word]] += 1\n",
    "    \n",
    "    if BOWType == 'Frequency':\n",
    "#         To accomodate for the fact that one of the rows has one word, d-gust-ing, and the lenght of the vector is zero.\n",
    "#         Hence, the vector returns [Nan Nan Nan ... Nan Nan].  Now it returns [0 0 0 ... 0 0 0]\n",
    "        vectorLength =  np.sum(vector)\n",
    "        if vectorLength>0:\n",
    "            vector = np.divide(vector, vectorLength)\n",
    "            \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createBagOfWordsMatrix(df, dictionary, BOWType):\n",
    "    translator = str.maketrans(string.punctuation.replace('\\'', ''), 31*' ', '\\'')\n",
    "    \n",
    "    if BOWType == 'BagOfWords':\n",
    "        distancesArray = np.zeros((len(df), len(dictionary)), dtype = np.int8)\n",
    "    elif BOWType == 'Frequency':\n",
    "        distancesArray = np.zeros((len(df), len(dictionary)))\n",
    "        \n",
    "    for i in range(0, len(df)):\n",
    "        vector = convertWordsToVector(df.iloc[i,0], dictionary, BOWType)\n",
    "        distancesArray[i] = vector\n",
    "        \n",
    "    return distancesArray\n",
    "\n",
    "# print(createBagOfWordsMatrix(readTxt('\\yelp-train.txt'), createDictionary('yelp'), 'Frequency'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createReviewsTxt(df, dictionary):\n",
    "    translator = str.maketrans(string.punctuation.replace('\\'', ''), 31*' ', '\\'')\n",
    "    reviewsList = list()\n",
    "    targets = df.iloc[:,1]\n",
    "    \n",
    "    for i in range(0, len(df)):\n",
    "        reviewsString = ''\n",
    "        trainingRow = df.iloc[i,0].translate(translator).lower().split(\" \")\n",
    "        for word in trainingRow:\n",
    "            if word in dictionary:\n",
    "                reviewsString += str(dictionary[word]) + ' ' \n",
    "        reviewsList.append(reviewsString)\n",
    "\n",
    "    outputDF = pd.DataFrame(list(zip(reviewsList, targets)))\n",
    "    toCsvDf('\\submit\\yelp-valid.txt', outputDF)\n",
    "    \n",
    "    return outputDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testScores(predicted, actual):\n",
    "    f1Score = f1_score(actual, predicted, average='macro')\n",
    "    confusionMatrix = confusion_matrix(actual, predicted)\n",
    "    print('F1 Score:', f1Score)\n",
    "    print('Confusion Matrix:\\n', confusionMatrix)\n",
    "    return f1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB\n",
      "{'alpha': 0.0256419, 'binarize': 0.0, 'class_prior': None, 'fit_prior': True}\n",
      "Training F1-Measure\n",
      "F1 Score: 0.873838532099\n",
      "Confusion Matrix:\n",
      " [[6666  834]\n",
      " [1058 6442]]\n",
      "Validation F1-Measure\n",
      "F1 Score: 0.845474285376\n",
      "Confusion Matrix:\n",
      " [[4292  708]\n",
      " [ 837 4163]]\n",
      "Test F1-Measure\n",
      "F1 Score: 0.834173134176\n",
      "Confusion Matrix:\n",
      " [[10801  1699]\n",
      " [ 2443 10057]]\n"
     ]
    }
   ],
   "source": [
    "def bernoulliNB(dataset, BOWType):\n",
    "    print(dataset)\n",
    "    if dataset == 'yelp':\n",
    "        dictionary = createDictionary('yelp')\n",
    "        trainingDF = readTxt('\\yelp-train.txt')\n",
    "        validDF = readTxt('\\yelp-valid.txt')\n",
    "        testDF = readTxt('\\yelp-test.txt')\n",
    "    elif dataset == 'IMDB':\n",
    "        dictionary = createDictionary('IMDB')\n",
    "        trainingDF = readTxt('\\IMDB-train.txt')\n",
    "        validDF = readTxt('\\IMDB-valid.txt')\n",
    "        testDF = readTxt('\\IMDB-test.txt')\n",
    "        \n",
    "    clf = BernoulliNB(alpha = 0.0256419)\n",
    "    print(clf.get_params())\n",
    "    clf.fit(createBagOfWordsMatrix(trainingDF, dictionary, BOWType), trainingDF[1])\n",
    "    \n",
    "    print('Training F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(trainingDF, dictionary, BOWType))\n",
    "    testScores(predictionsArray, trainingDF[1])\n",
    "    \n",
    "    print('Validation F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(validDF, dictionary, BOWType))\n",
    "    testScores(predictionsArray, validDF[1])\n",
    "    \n",
    "    print('Test F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(testDF, dictionary, BOWType))\n",
    "    testScores(predictionsArray, testDF[1])\n",
    "    \n",
    "bernoulliNB('IMDB', 'Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB\n",
      "{'C': 100, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 5758, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.01, 'verbose': 0}\n",
      "Training F1-Measure\n",
      "F1 Score: 0.948599537396\n",
      "Confusion Matrix:\n",
      " [[7092  408]\n",
      " [ 363 7137]]\n",
      "Validation F1-Measure\n",
      "F1 Score: 0.882395479282\n",
      "Confusion Matrix:\n",
      " [[4381  619]\n",
      " [ 557 4443]]\n",
      "Test F1-Measure\n",
      "F1 Score: 0.877359038495\n",
      "Confusion Matrix:\n",
      " [[10932  1568]\n",
      " [ 1498 11002]]\n"
     ]
    }
   ],
   "source": [
    "def linearSVC(dataset, BOWType):\n",
    "    print(dataset)\n",
    "    if dataset == 'yelp':\n",
    "        dictionary = createDictionary('yelp')\n",
    "        trainingDF = readTxt('\\yelp-train.txt')\n",
    "        validDF = readTxt('\\yelp-valid.txt')\n",
    "        testDF = readTxt('\\yelp-test.txt')\n",
    "    elif dataset == 'IMDB':\n",
    "        dictionary = createDictionary('IMDB')\n",
    "        trainingDF = readTxt('\\IMDB-train.txt')\n",
    "        validDF = readTxt('\\IMDB-valid.txt')\n",
    "        testDF = readTxt('\\IMDB-test.txt')\n",
    "        \n",
    "    clf = LinearSVC(C=100, dual = False, max_iter = 5758, tol = 0.01)\n",
    "    print(clf.get_params())\n",
    "    clf.fit(createBagOfWordsMatrix(trainingDF, dictionary, BOWType), trainingDF[1])\n",
    "    \n",
    "    print('Training F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(trainingDF, dictionary, BOWType))\n",
    "    testScores(predictionsArray, trainingDF[1])\n",
    "    \n",
    "    print('Validation F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(validDF, dictionary, BOWType))\n",
    "    testScores(predictionsArray, validDF[1])\n",
    "    \n",
    "    print('Test F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(testDF, dictionary, BOWType))\n",
    "    testScores(predictionsArray, testDF[1])\n",
    "    \n",
    "linearSVC('IMDB', 'Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB\n",
      "{'class_weight': None, 'criterion': 'gini', 'max_depth': 23, 'max_features': None, 'max_leaf_nodes': 351, 'min_impurity_decrease': 0.0003671190352878896, 'min_impurity_split': None, 'min_samples_leaf': 16, 'min_samples_split': 195, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': None, 'splitter': 'best'}\n",
      "Training F1-Measure\n",
      "F1 Score: 0.791918478527\n",
      "Confusion Matrix:\n",
      " [[5700 1800]\n",
      " [1318 6182]]\n",
      "Validation F1-Measure\n",
      "F1 Score: 0.726264209885\n",
      "Confusion Matrix:\n",
      " [[3434 1566]\n",
      " [1167 3833]]\n",
      "Test F1-Measure\n",
      "F1 Score: 0.740742472922\n",
      "Confusion Matrix:\n",
      " [[8787 3713]\n",
      " [2759 9741]]\n"
     ]
    }
   ],
   "source": [
    "def decisionTree(dataset, BOWType):\n",
    "    print(dataset)\n",
    "    if dataset == 'yelp':\n",
    "        dictionary = createDictionary('yelp')\n",
    "        trainingDF = readTxt('\\yelp-train.txt')\n",
    "        validDF = readTxt('\\yelp-valid.txt')\n",
    "        testDF = readTxt('\\yelp-test.txt')\n",
    "    elif dataset == 'IMDB':\n",
    "        dictionary = createDictionary('IMDB')\n",
    "        trainingDF = readTxt('\\IMDB-train.txt')\n",
    "        validDF = readTxt('\\IMDB-valid.txt')\n",
    "        testDF = readTxt('\\IMDB-test.txt')\n",
    "        \n",
    "    clf = tree.DecisionTreeClassifier(max_depth = 23, \n",
    "                                      max_leaf_nodes= 351, min_impurity_decrease = 0.00036711903528788959, \n",
    "                                      min_samples_leaf = 16, min_samples_split = 195)\n",
    "    print(clf.get_params())\n",
    "    clf.fit(createBagOfWordsMatrix(trainingDF, dictionary, BOWType), trainingDF[1])\n",
    "    \n",
    "    print('Training F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(trainingDF, dictionary, BOWType))\n",
    "    testScores(predictionsArray, trainingDF[1])\n",
    "    \n",
    "    print('Validation F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(validDF, dictionary, BOWType))\n",
    "    testScores(predictionsArray, validDF[1])\n",
    "    \n",
    "    print('Test F1-Measure')\n",
    "    predictionsArray = clf.predict(createBagOfWordsMatrix(testDF, dictionary, BOWType))\n",
    "    testScores(predictionsArray, testDF[1])\n",
    "    \n",
    "decisionTree('IMDB', 'Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB Random Classifier\n",
      "Training F1-Measure\n",
      "F1 Score: 0.505866666667\n",
      "Confusion Matrix:\n",
      " [[3794 3706]\n",
      " [3706 3794]]\n",
      "Validation F1-Measure\n",
      "F1 Score: 0.49864043347\n",
      "Confusion Matrix:\n",
      " [[2439 2561]\n",
      " [2452 2548]]\n",
      "Test F1-Measure\n",
      "F1 Score: 0.500239996802\n",
      "Confusion Matrix:\n",
      " [[6254 6246]\n",
      " [6248 6252]]\n"
     ]
    }
   ],
   "source": [
    "def randomClassifier(dataset, BOWType):\n",
    "    print(dataset, 'Random Classifier')\n",
    "    if dataset == 'yelp':\n",
    "        dictionary = createDictionary('yelp')\n",
    "        trainingDF = readTxt('\\yelp-train.txt')\n",
    "        validDF = readTxt('\\yelp-valid.txt')\n",
    "        testDF = readTxt('\\yelp-test.txt')\n",
    "    elif dataset == 'IMDB':\n",
    "        dictionary = createDictionary('IMDB')\n",
    "        trainingDF = readTxt('\\IMDB-train.txt')\n",
    "        validDF = readTxt('\\IMDB-valid.txt')\n",
    "        testDF = readTxt('\\IMDB-test.txt')\n",
    "    \n",
    "    randomClassifier = DummyClassifier(strategy='uniform')\n",
    "    randomClassifier.fit(createBagOfWordsMatrix(trainingDF, dictionary, BOWType), trainingDF[1])\n",
    "    \n",
    "    print('Training F1-Measure')\n",
    "    predictionsArray = randomClassifier.predict(createBagOfWordsMatrix(trainingDF, dictionary, BOWType))\n",
    "    testScores(predictionsArray, trainingDF[1])\n",
    "    \n",
    "    print('Validation F1-Measure')\n",
    "    predictionsArray = randomClassifier.predict(createBagOfWordsMatrix(validDF, dictionary, BOWType))\n",
    "    testScores(predictionsArray, validDF[1])\n",
    "    \n",
    "    print('Test F1-Measure')\n",
    "    predictionsArray = randomClassifier.predict(createBagOfWordsMatrix(testDF, dictionary, BOWType))\n",
    "    testScores(predictionsArray, testDF[1])\n",
    "    \n",
    "randomClassifier('IMDB', 'BagOfWords')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
